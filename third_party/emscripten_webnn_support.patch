diff --git a/emcc.py b/emcc.py
index beaddc73b..6a107346f 100755
--- a/emcc.py
+++ b/emcc.py
@@ -1826,6 +1826,7 @@ There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR P
     if shared.Settings.RELOCATABLE or \
        shared.Settings.BUILD_AS_WORKER or \
        shared.Settings.USE_WEBGPU or \
+       shared.Settings.USE_WEBNN or \
        shared.Settings.USE_PTHREADS or \
        shared.Settings.OFFSCREENCANVAS_SUPPORT or \
        shared.Settings.LEGACY_GL_EMULATION or \
diff --git a/src/library_html5_webnn.js b/src/library_html5_webnn.js
new file mode 100644
index 000000000..adf80e0fe
--- /dev/null
+++ b/src/library_html5_webnn.js
@@ -0,0 +1,8 @@
+mergeInto(LibraryManager.library, {
+  emscripten_webnn_create_context__deps: ['$WebNN'],
+  emscripten_webnn_create_context__postset: 'WebNN.initManagers();',
+  emscripten_webnn_create_context: function() {
+    var context = navigator.ml.createContext();
+    return WebNN.mgrContext.create(context);
+  },
+});
diff --git a/src/library_webnn.js b/src/library_webnn.js
new file mode 100644
index 000000000..b95d5436c
--- /dev/null
+++ b/src/library_webnn.js
@@ -0,0 +1,541 @@
+/**
+ * @license
+ * Copyright 2021 The Emscripten Authors
+ * SPDX-License-Identifier: MIT
+ */
+
+/*
+ * WebNN support.
+ *
+ * This file implements the common C header <webnn/webnn.h> on top of the
+ * browser's native JS WebNN implementation. This allows applications targeting
+ * webnn-native (https://github.com/webmachinelearning/webnn-native) also target the Web with the
+ * same API and fairly minimal changes - similar to OpenGL ES 2.0/3.0
+ * on WebGL 1.0/2.0.
+ */
+
+{{{ (function() {
+  // Helper functions for code generation
+  global.webnn = {
+    makeInitManager: function(type) {
+      var mgr = 'this.mgr' + type
+      return mgr + ' = ' + mgr + ' || makeManager();';
+    },
+    makeReferenceRelease: function(type) {
+      var s = '';
+      s += 'ml' + type + 'Reference: function(id) {\n';
+      s += '  WebNN.mgr' + type + '.reference(id);\n'
+      s += '},\n';
+      s += 'ml' + type + 'Release: function(id) {\n';
+      s += '  WebNN.mgr' + type + '.release(id);\n'
+      s += '},';
+      return s;
+    },
+    makeU64ToNumber: function(lowName, highName) {
+      var ret = '('
+      if (ASSERTIONS) {
+        ret += 'assert(' + highName + ' < 0x200000), ';
+      }
+      ret += highName + ' * 0x100000000 + ' + lowName + ')\n'
+      return ret;
+    },
+    makeGetBool: function(struct, offset) {
+      // In an actual build, bool seems to be i8. But on the off-chance it's i32, on little-endian
+      // this will still work as long as the value of 'true' isn't zero in the lowest byte.
+      return '(' + makeGetValue(struct, offset, 'i8') + ' !== 0)';
+    },
+    makeGetU32: function(struct, offset) {
+      return makeGetValue(struct, offset, 'i32', false, true);
+    },
+    makeGetI32: function(struct, offset) {
+      return makeGetValue(struct, offset, 'i32', false, false);
+    },
+    makeGetF32: function(struct, offset) {
+      return makeGetValue(struct, offset, 'float');
+    },
+    makeGetU64: function(struct, offset) {
+      var l = makeGetValue(struct, offset, 'i32', false, true);
+      var h = makeGetValue('(' + struct + ' + 4)', offset, 'i32', false, true)
+      return h + ' * 0x100000000 + ' + l
+    },
+    makeCheck: function(str) {
+      if (!ASSERTIONS) return '';
+      return 'assert(' + str + ');';
+    },
+    makeCheckDefined: function(name) {
+      return this.makeCheck('typeof ' + name + ' !== "undefined"');
+    },
+  };
+  return null;
+})(); }}}
+
+var LibraryWebNN = {
+  $WebNN: {
+    initManagers: function() {
+      if (this["mgrContext"]) return;
+
+      function makeManager() {
+        return {
+          objects: {},
+          nextId: 1,
+          create: function(object, wrapper /* = {} */) {
+            wrapper = wrapper || {};
+
+            var id = this.nextId++;
+            {{{ webnn.makeCheck("typeof this.objects[id] === 'undefined'") }}}
+            wrapper.refcount = 1;
+            wrapper.object = object;
+            this.objects[id] = wrapper;
+            return id;
+          },
+          get: function(id) {
+            if (id === 0) return undefined;
+            var o = this.objects[id];
+            {{{ webnn.makeCheckDefined('o') }}}
+            return o.object;
+          },
+          reference: function(id) {
+            var o = this.objects[id];
+            {{{ webnn.makeCheckDefined('o') }}}
+            o.refcount++;
+          },
+          release: function(id) {
+            var o = this.objects[id];
+            {{{ webnn.makeCheckDefined('o') }}}
+            {{{ webnn.makeCheck('o.refcount > 0') }}}
+            o.refcount--;
+            if (o.refcount <= 0) {
+              delete this.objects[id];
+            }
+          },
+        };
+      }
+
+      this["mgrContext"] = this["mgrContext"] || makeManager();
+      {{{ webnn.makeInitManager('Graph') }}}
+      {{{ webnn.makeInitManager('GraphBuilder') }}}
+      {{{ webnn.makeInitManager('NamedInputs') }}}
+      {{{ webnn.makeInitManager('NamedOperands') }}}
+      {{{ webnn.makeInitManager('NamedOutputs') }}}
+      {{{ webnn.makeInitManager('NamedResults') }}}
+      {{{ webnn.makeInitManager('Operand') }}}
+      {{{ webnn.makeInitManager('Result') }}}
+    },
+
+    AutoPad: [
+      'explicit',
+      'same-upper',
+      'same-lower',
+    ],
+    BuildGraphStatus: [
+      'success',
+      'error',
+      'context-lost',
+      'unknown',
+    ],
+    ComputeGraphStatus: [
+      'success',
+      'error',
+      'context-lost',
+      'unknown',
+    ],
+    ErrorFilter: [
+      'none',
+      'validation',
+      'out-of-memory',
+    ],
+    ErrorType: [
+      'no-error',
+      'validation',
+      'out-of-memory',
+      'unknown',
+      'device-lost',
+    ],
+    FilterOperandLayout: [
+      'oihw',
+      'hwio',
+      'ohwi',
+      'ihwo',
+    ],
+    FusedActivation: [
+      'none',
+      'relu',
+    ],
+    InputOperandLayout: [
+      'nchw',
+      'nhwc',
+    ],
+    OperandType: [
+      'float32',
+      'float16',
+      'int32',
+      'uint32',
+      'int8',
+      'uint8',
+    ],
+    PowerPreference: [
+      'default',
+      'high_performance',
+      'low_power',
+    ],
+
+    makeI32Array: function(count, arrayPtr) {
+      if (count === 0 || arrayPtr === 0) {
+        return undefined;
+      }
+      var array = [];
+      for (var i = 0; i < count; ++i, arrayPtr += 4) {
+        array.push({{{ webnn.makeGetI32('arrayPtr', 0) }}});
+      }
+      return array;
+    },
+
+    makeArrayBufferView: function(offset, byteSize, type = "float32") {
+      assert(type === "float32");
+      // TODO: support other array buffer view types.
+      return new Float32Array(HEAPU8.buffer, offset, byteSize / Float32Array.BYTES_PER_ELEMENT);
+    },
+
+    makeClampOptions: function(ptr) {
+      return {
+        "minValue": this.mgrOperand.get({{{ makeGetValue('ptr', C_STRUCTS.MLClampOptions.minValue, '*') }}}),
+        "maxValue": this.mgrOperand.get({{{ makeGetValue('ptr', C_STRUCTS.MLClampOptions.maxValue, '*') }}}),
+      };
+    },
+
+    makeBatchNormOptions: function(ptr) {
+      return {
+        "scale": this.mgrOperand.get({{{ makeGetValue('ptr', C_STRUCTS.MLBatchNormOptions.scale, '*') }}}),
+        "bias": this.mgrOperand.get({{{ makeGetValue('ptr', C_STRUCTS.MLBatchNormOptions.bias, '*') }}}),
+        "axis": {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLBatchNormOptions.axis) }}},
+        "epsilon": {{{ webnn.makeGetF32('ptr', C_STRUCTS.MLBatchNormOptions.epsilon) }}},
+        "activation": this.FusedActivation[ {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLBatchNormOptions.activation) }}}],
+      };
+    },
+
+    makeGemmOptions: function(ptr) {
+      return {
+        "c": this.mgrOperand.get({{{ makeGetValue('ptr', C_STRUCTS.MLGemmOptions.c, '*') }}}),
+        "alpha": {{{ webnn.makeGetF32('ptr', C_STRUCTS.MLGemmOptions.alpha) }}},
+        "beta": {{{ webnn.makeGetF32('ptr', C_STRUCTS.MLGemmOptions.beta) }}},
+        "aTranspose": {{{ webnn.makeGetBool('ptr', C_STRUCTS.MLGemmOptions.aTranspose)}}},
+        "bTranspose": {{{ webnn.makeGetBool('ptr', C_STRUCTS.MLGemmOptions.bTranspose)}}},
+      };
+    },
+
+    makeOperandDescriptor: function(ptr) {
+      return {
+        "type": this.OperandType[
+            {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLOperandDescriptor.type) }}}
+        ],
+        "dimensions": this.makeI32Array(
+            {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLOperandDescriptor.dimensionsCount) }}},
+            {{{ makeGetValue('ptr', C_STRUCTS.MLOperandDescriptor.dimensions, '*') }}}
+        ),
+      };
+    },
+    
+    makeConv2dOptions: function(ptr) {
+      return {
+        "padding": this.AutoPad[
+            {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLConv2dOptions.autoPad) }}}
+          ] === 'explicit' ? this.makeI32Array(
+            {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLConv2dOptions.paddingCount) }}},
+            {{{ makeGetValue('ptr', C_STRUCTS.MLConv2dOptions.padding, '*') }}}
+          ) : undefined,
+        "strides": this.makeI32Array(
+          {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLConv2dOptions.stridesCount) }}},
+          {{{ makeGetValue('ptr', C_STRUCTS.MLConv2dOptions.strides, '*') }}}
+        ),
+        "dilations": this.makeI32Array(
+          {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLConv2dOptions.dilationsCount) }}},
+          {{{ makeGetValue('ptr', C_STRUCTS.MLConv2dOptions.dilations, '*') }}}
+        ),
+        "autoPad": this.AutoPad[
+          {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLConv2dOptions.autoPad) }}}
+        ],
+        "groups": {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLConv2dOptions.groups) }}},
+        "inputLayout": this.InputOperandLayout[
+          {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLConv2dOptions.inputLayout) }}}
+        ],
+        "filterLayout": this.FilterOperandLayout[
+          {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLConv2dOptions.filterLayout) }}}
+        ],
+        "bias": this.mgrOperand.get({{{ makeGetValue('ptr', C_STRUCTS.MLConv2dOptions.bias, '*') }}}),
+        "activation": this.FusedActivation[ {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLConv2dOptions.activation) }}}],
+      };
+    },
+
+    makePool2dOptions: function(ptr) {
+      return {
+        "windowDimensions": this.makeI32Array(
+          {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLPool2dOptions.windowDimensionsCount) }}},
+          {{{ makeGetValue('ptr', C_STRUCTS.MLPool2dOptions.windowDimensions, '*') }}}
+        ),
+        "padding": this.AutoPad[
+            {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLPool2dOptions.autoPad) }}}
+          ] === 'explicit' ? this.makeI32Array(
+            {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLPool2dOptions.paddingCount) }}},
+            {{{ makeGetValue('ptr', C_STRUCTS.MLPool2dOptions.padding, '*') }}}
+          ) : undefined,
+        "strides": this.makeI32Array(
+          {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLPool2dOptions.stridesCount) }}},
+          {{{ makeGetValue('ptr', C_STRUCTS.MLPool2dOptions.strides, '*') }}}
+        ),
+        "dilations": this.makeI32Array(
+          {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLPool2dOptions.dilationsCount) }}},
+          {{{ makeGetValue('ptr', C_STRUCTS.MLPool2dOptions.dilations, '*') }}}
+        ),
+        "autoPad": this.AutoPad[
+          {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLPool2dOptions.autoPad) }}}
+        ],
+        "layout": this.InputOperandLayout[
+          {{{ webnn.makeGetI32('ptr', C_STRUCTS.MLPool2dOptions.layout) }}}
+        ],
+      };
+    },
+
+    makeInput: function(ptr) {
+      return {
+        "data": this.makeArrayBufferView(
+            {{{ makeGetValue('ptr', C_STRUCTS.MLInput.buffer, '*') }}}, 
+            {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLInput.size) }}}
+        ),
+        "dimensions":({{{ makeGetValue('ptr', C_STRUCTS.MLInput.dimensions, '*') }}} === 0) ? undefined :
+            this.makeI32Array(
+                {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLInput.dimensionsCount) }}},
+                {{{ makeGetValue('ptr', C_STRUCTS.MLInput.dimensions, '*') }}}
+        ),
+      };
+    },
+
+    makeOutput: function(ptr) {
+      return {
+        "data": ({{{ makeGetValue('ptr', C_STRUCTS.MLOutput.buffer, '*') }}} === 0) ? undefined :
+            this.makeArrayBufferView(
+                {{{ makeGetValue('ptr', C_STRUCTS.MLOutput.buffer, '*') }}}, 
+                {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLOutput.size) }}}
+        ),
+        "dimensions":({{{ makeGetValue('ptr', C_STRUCTS.MLOutput.dimensions, '*') }}} === 0) ? undefined :
+            this.makeI32Array(
+                {{{ webnn.makeGetU32('ptr', C_STRUCTS.MLOutput.dimensionsCount) }}},
+                {{{ makeGetValue('ptr', C_STRUCTS.MLOutput.dimensions, '*') }}}
+        ),
+      };
+    },
+  },
+
+  // *Reference/*Release
+
+  {{{ webnn.makeReferenceRelease('Graph') }}}
+  {{{ webnn.makeReferenceRelease('GraphBuilder') }}}
+  {{{ webnn.makeReferenceRelease('NamedInputs') }}}
+  {{{ webnn.makeReferenceRelease('NamedOperands') }}}
+  {{{ webnn.makeReferenceRelease('NamedOutputs') }}}
+  {{{ webnn.makeReferenceRelease('NamedResults') }}}
+  {{{ webnn.makeReferenceRelease('Context') }}}
+  {{{ webnn.makeReferenceRelease('Operand') }}}
+  {{{ webnn.makeReferenceRelease('Result') }}}
+
+  // Methods of GraphBuilder
+
+  mlGraphBuilderInput: function(builderId, namePtr, descPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var name = UTF8ToString(namePtr);
+    var desc = WebNN.makeOperandDescriptor(descPtr);
+    var input = builder.input(name, desc);
+    return WebNN.mgrOperand.create(input);
+  },
+
+  mlGraphBuilderConstant: function(builderId, descPtr, valuePtr, size) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var desc = WebNN.makeOperandDescriptor(descPtr);
+    var buffer = WebNN.makeArrayBufferView(valuePtr, size);
+    var constant = builder.constant(desc, buffer);
+    return WebNN.mgrOperand.create(constant);
+  },
+
+  mlGraphBuilderAveragePool2d: function(builderId, inputId, optionsPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var options = WebNN.makePool2dOptions(optionsPtr);
+    var pool2d = builder.averagePool2d(input, options);
+    return WebNN.mgrOperand.create(pool2d);
+  },
+
+  mlGraphBuilderMaxPool2d: function(builderId, inputId, optionsPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var options = WebNN.makePool2dOptions(optionsPtr);
+    var pool2d = builder.maxPool2d(input, options);
+    return WebNN.mgrOperand.create(pool2d);
+  },
+
+  mlGraphBuilderBatchNorm: function(builderId, inputId, meanId, varianceId, optionsPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var mean = WebNN.mgrOperand.get(meanId);
+    var variance = WebNN.mgrOperand.get(varianceId);
+    var options = WebNN.makeBatchNormOptions(optionsPtr);
+    var output = builder.batchNormalization(input, mean, variance, options);
+    return WebNN.mgrOperand.create(output);
+  },
+
+  mlGraphBuilderConcat: function(builderId, inputsCount, inputsPtr, axis) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var inputIds = WebNN.makeI32Array(inputsCount, inputsPtr);
+    var inputs = [];
+    for (var i = 0; i < inputIds.length; ++i) {
+      inputs.push(WebNN.mgrOperand.get(inputIds[i]));
+    }
+    var output = builder.concat(inputs, axis);
+    return WebNN.mgrOperand.create(output);
+  },
+
+  mlGraphBuilderGemm: function(builderId, aId, bId, optionsPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var a = WebNN.mgrOperand.get(aId);
+    var b = WebNN.mgrOperand.get(bId);
+    var options = WebNN.makeGemmOptions(optionsPtr);
+    var output = builder.gemm(a, b, options);
+    return WebNN.mgrOperand.create(output);
+  },
+
+  mlGraphBuilderReshape: function(builderId, inputId, newShapePtr, newShapeCount) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var newShape = WebNN.makeI32Array(newShapeCount, newShapePtr);
+    var output = builder.reshape(input, newShape);
+    return WebNN.mgrOperand.create(output);
+  },
+
+  mlGraphBuilderMatmul: function(builderId, aId, bId) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var a = WebNN.mgrOperand.get(aId);
+    var b = WebNN.mgrOperand.get(bId);
+    var c = builder.matmul(a, b);
+    return WebNN.mgrOperand.create(c);
+  },
+
+  mlGraphBuilderRelu: function(builderId, inputId) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var output = builder.relu(input);
+    return WebNN.mgrOperand.create(output);
+  },
+
+  mlGraphBuilderConv2d: function(builderId, inputId, filterId, optionsPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var filter = WebNN.mgrOperand.get(filterId);
+    var options = WebNN.makeConv2dOptions(optionsPtr);
+    var conv2d = builder.conv2d(input, filter, options);
+    return WebNN.mgrOperand.create(conv2d);
+  },
+
+  mlGraphBuilderClamp: function(builderId, inputId, optionsPtr) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var input = WebNN.mgrOperand.get(inputId);
+    var options = WebNN.makeClampOptions(optionsPtr);
+    var clamp = builder.clamp(input, options);
+    return WebNN.mgrOperand.create(clamp);
+  },
+
+  mlGraphBuilderAdd: function(builderId, aId, bId) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var a = WebNN.mgrOperand.get(aId);
+    var b = WebNN.mgrOperand.get(bId);
+    var c = builder.add(a, b);
+    return WebNN.mgrOperand.create(c);
+  },
+
+  webnnCreateNamedInputs: function() {
+    var inputs = {};
+    return WebNN.mgrNamedInputs.create(inputs);
+  },
+
+  mlNamedInputsSet: function(namedInputsId, namePtr, inputPtr) {
+    var namedInputs = WebNN.mgrNamedInputs.get(namedInputsId);
+    var name = UTF8ToString(namePtr);
+    var input = WebNN.makeInput(inputPtr);
+    namedInputs[name] = input;
+  },
+
+  webnnCreateNamedOutputs: function() {
+    var outputs = {};
+    return WebNN.mgrNamedOutputs.create(outputs);
+  },
+
+  mlNamedOutputsSet: function(namedOutputsId, namePtr, outputPtr) {
+    var namedOutputs = WebNN.mgrNamedOutputs.get(namedOutputsId);
+    var name = UTF8ToString(namePtr);
+    var output = WebNN.makeOutput(outputPtr);
+    namedOutputs[name] = output;
+  },
+
+  webnnCreateNamedOperands: function() {
+    var operands = {};
+    return WebNN.mgrNamedOperands.create(operands);
+  },
+
+  mlNamedOperandsSet: function(namedOperandsId, namePtr, operandId) {
+    var namedOperands = WebNN.mgrNamedOperands.get(namedOperandsId);
+    var name = UTF8ToString(namePtr);
+    var operand = WebNN.mgrOperand.get(operandId);
+    namedOperands[name] = operand;
+  },
+
+  webnnCreateGraphBuilder: function(contextId) {
+    var context = WebNN.mgrContext.get(contextId);
+    var builder = new MLGraphBuilder(context);
+    return WebNN.mgrGraphBuilder.create(builder);
+  },
+
+  mlGraphBuilderBuild: function(builderId, namedOperandsId, callback, userdata) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var namedOperands = WebNN.mgrNamedOperands.get(namedOperandsId);
+    builder.build(namedOperands).then(function(graph) {
+      var graphId = WebNN.mgrGraph.create(graph);
+      {{{ makeDynCall('viiii', 'callback') }}}(0 /* MLBuildGraphStatus_Success */, graphId, 0, userdata);
+    }, function(error) {
+      var messagePtr = allocateUTF8(error.message);
+      {{{ makeDynCall('viiii', 'callback') }}}(1 /* MLBuildGraphStatus_Error */, 0, messagePtr, userdata);
+    });
+  },
+
+  mlGraphBuilderBuildSync: function(builderId, namedOperandsId) {
+    var builder = WebNN.mgrGraphBuilder.get(builderId);
+    var namedOperands = WebNN.mgrNamedOperands.get(namedOperandsId);
+    try {
+      var graph = builder.buildSync(namedOperands);
+      return WebNN.mgrGraph.create(graph);
+    } catch (error) {
+      console.log('builder.buildSync failed: ' + error);
+      return 0;  // nullptr
+    }
+  },
+
+  mlGraphCompute: function(graphId, inputsId, callback, userdata, outputsId) {
+    var graph = WebNN.mgrGraph.get(graphId);
+    var inputs = WebNN.mgrNamedInputs.get(inputsId);
+    var outputs = outputsId === 0 ? undefined : WebNN.mgrNamedOutputs.get(outputsId);
+    graph.compute(inputs, outputs).then(function(results) {
+      // TODO: implement results
+      {{{ makeDynCall('viiii', 'callback') }}}(0 /* MLComputeGraphStatus_Success */, 0, 0, userdata);
+    }, function(error) {
+      var messagePtr = allocateUTF8(error.message);
+      {{{ makeDynCall('viiii', 'callback') }}}(1 /* MLComputeGraphStatus_Error */, 0, messagePtr, userdata);
+    });
+  },
+
+  mlGraphComputeSync: function(graphId, inputsId, outputsId) {
+    var graph = WebNN.mgrGraph.get(graphId);
+    var inputs = WebNN.mgrNamedInputs.get(inputsId);
+    var outputs = WebNN.mgrNamedOutputs.get(outputsId);
+    return graph.computeSync(inputs, outputs);
+  },
+
+};
+
+autoAddDeps(LibraryWebNN, '$WebNN');
+mergeInto(LibraryManager.library, LibraryWebNN);
diff --git a/src/modules.js b/src/modules.js
index 0a94d52ea..fa1eb1314 100644
--- a/src/modules.js
+++ b/src/modules.js
@@ -164,6 +164,11 @@ var LibraryManager = {
       libraries.push('library_html5_webgpu.js');
     }
 
+    if (USE_WEBNN) {
+      libraries.push('library_webnn.js');
+      libraries.push('library_html5_webnn.js');
+    }
+
     if (BOOTSTRAPPING_STRUCT_INFO) {
       libraries = [
         'library_bootstrap.js',
diff --git a/src/settings.js b/src/settings.js
index eb43818c9..80267c4a1 100644
--- a/src/settings.js
+++ b/src/settings.js
@@ -546,6 +546,10 @@ var GL_PREINITIALIZED_CONTEXT = 0;
 // [link]
 var USE_WEBGPU = 0;
 
+// Enables support for WebNN (via "webnn/webnn.h").
+// [link]
+var USE_WEBNN = 0;
+
 // Enables building of stb-image, a tiny public-domain library for decoding
 // images, allowing decoding of images without using the browser's built-in
 // decoders. The benefit is that this can be done synchronously, however, it
diff --git a/src/struct_info.json b/src/struct_info.json
index 84793c90c..97cade861 100644
--- a/src/struct_info.json
+++ b/src/struct_info.json
@@ -1539,6 +1539,83 @@
             ]
         }
     },
+    {
+        "file": "webnn/webnn.h",
+        "defines": [],
+        "structs": {
+            "MLBatchNormOptions": [
+                "scale",
+                "bias",
+                "axis",
+                "epsilon",
+                "activation"
+            ],
+            "MLClampOptions": [
+                "minValue",
+                "maxValue"
+            ],
+            "MLContextOptions": [
+                "powerPreference"
+            ],
+            "MLConv2dOptions": [
+                "paddingCount",
+                "padding",
+                "stridesCount",
+                "strides",
+                "dilationsCount",
+                "dilations",
+                "autoPad",
+                "groups",
+                "inputLayout",
+                "filterLayout",
+                "bias",
+                "activation"
+            ],
+            "MLGemmOptions": [
+                "c",
+                "alpha",
+                "beta",
+                "aTranspose",
+                "bTranspose"
+            ],
+            "MLInput": [
+                "buffer",
+                "size",
+                "dimensions",
+                "dimensionsCount"
+            ],
+            "MLLeakyReluOptions": [
+                "alpha"
+            ],
+            "MLOperandDescriptor": [
+                "type",
+                "dimensions",
+                "dimensionsCount"
+            ],
+            "MLOutput": [
+                "buffer",
+                "size",
+                "dimensions",
+                "dimensionsCount"
+            ],
+            "MLPool2dOptions": [
+                "windowDimensionsCount",
+                "windowDimensions",
+                "paddingCount",
+                "padding",
+                "stridesCount",
+                "strides",
+                "dilationsCount",
+                "dilations",
+                "autoPad",
+                "layout"
+            ],
+            "MLTransposeOptions": [
+                "permutationCount",
+                "permutation"
+            ]
+        }
+    },
     // ===========================================
     // WebGPU
     //   NOTE: This can be (mostly) auto-generated:
diff --git a/src/worker.js b/src/worker.js
index 1a9ff901c..84d6e44f2 100644
--- a/src/worker.js
+++ b/src/worker.js
@@ -232,7 +232,7 @@ this.onmessage = function(e) {
           // FIXME(sbc): Figure out if this is still needed or useful.  Its not
           // clear to me how this check could ever fail.  In order to get into
           // this try/catch block at all we have already called bunch of
-          // functions on `Module`.. why is this one special?
+          // functions on Module.. why is this one special?
           if (typeof(Module['_emscripten_futex_wake']) !== "function") {
             err("Thread Initialisation failed.");
             throw ex;
diff --git a/system/include/emscripten/html5_webnn.h b/system/include/emscripten/html5_webnn.h
new file mode 100644
index 000000000..9192c4d1b
--- /dev/null
+++ b/system/include/emscripten/html5_webnn.h
@@ -0,0 +1,20 @@
+/*
+ * Copyright 2021 The Emscripten Authors.  All rights reserved.
+ * Emscripten is available under two separate licenses, the MIT license and the
+ * University of Illinois/NCSA Open Source License.  Both these licenses can be
+ * found in the LICENSE file.
+ */
+
+#pragma once
+
+#include <webnn/webnn.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+MLContext emscripten_webnn_create_context(void);
+
+#ifdef __cplusplus
+} // ~extern "C"
+#endif
\ No newline at end of file
diff --git a/system/include/webnn/EnumClassBitmasks.h b/system/include/webnn/EnumClassBitmasks.h
new file mode 100644
index 000000000..bdef1df3d
--- /dev/null
+++ b/system/include/webnn/EnumClassBitmasks.h
@@ -0,0 +1,145 @@
+// Copyright 2017 The Dawn Authors
+// Copyright 2021 The WebNN-native Authors
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef WEBNN_ENUM_CLASS_BITMASKS_H_
+#define WEBNN_ENUM_CLASS_BITMASKS_H_
+
+#include <type_traits>
+
+namespace webnn {
+
+    template <typename T>
+    struct IsDawnBitmask {
+        static constexpr bool enable = false;
+    };
+
+    template <typename T, typename Enable = void>
+    struct LowerBitmask {
+        static constexpr bool enable = false;
+    };
+
+    template <typename T>
+    struct LowerBitmask<T, typename std::enable_if<IsDawnBitmask<T>::enable>::type> {
+        static constexpr bool enable = true;
+        using type = T;
+        constexpr static T Lower(T t) {
+            return t;
+        }
+    };
+
+    template <typename T>
+    struct BoolConvertible {
+        using Integral = typename std::underlying_type<T>::type;
+
+        constexpr BoolConvertible(Integral value) : value(value) {
+        }
+        constexpr operator bool() const {
+            return value != 0;
+        }
+        constexpr operator T() const {
+            return static_cast<T>(value);
+        }
+
+        Integral value;
+    };
+
+    template <typename T>
+    struct LowerBitmask<BoolConvertible<T>> {
+        static constexpr bool enable = true;
+        using type = T;
+        static constexpr type Lower(BoolConvertible<T> t) {
+            return t;
+        }
+    };
+
+    template <typename T1,
+              typename T2,
+              typename = typename std::enable_if<LowerBitmask<T1>::enable &&
+                                                 LowerBitmask<T2>::enable>::type>
+    constexpr BoolConvertible<typename LowerBitmask<T1>::type> operator|(T1 left, T2 right) {
+        using T = typename LowerBitmask<T1>::type;
+        using Integral = typename std::underlying_type<T>::type;
+        return static_cast<Integral>(LowerBitmask<T1>::Lower(left)) |
+               static_cast<Integral>(LowerBitmask<T2>::Lower(right));
+    }
+
+    template <typename T1,
+              typename T2,
+              typename = typename std::enable_if<LowerBitmask<T1>::enable &&
+                                                 LowerBitmask<T2>::enable>::type>
+    constexpr BoolConvertible<typename LowerBitmask<T1>::type> operator&(T1 left, T2 right) {
+        using T = typename LowerBitmask<T1>::type;
+        using Integral = typename std::underlying_type<T>::type;
+        return static_cast<Integral>(LowerBitmask<T1>::Lower(left)) &
+               static_cast<Integral>(LowerBitmask<T2>::Lower(right));
+    }
+
+    template <typename T1,
+              typename T2,
+              typename = typename std::enable_if<LowerBitmask<T1>::enable &&
+                                                 LowerBitmask<T2>::enable>::type>
+    constexpr BoolConvertible<typename LowerBitmask<T1>::type> operator^(T1 left, T2 right) {
+        using T = typename LowerBitmask<T1>::type;
+        using Integral = typename std::underlying_type<T>::type;
+        return static_cast<Integral>(LowerBitmask<T1>::Lower(left)) ^
+               static_cast<Integral>(LowerBitmask<T2>::Lower(right));
+    }
+
+    template <typename T1>
+    constexpr BoolConvertible<typename LowerBitmask<T1>::type> operator~(T1 t) {
+        using T = typename LowerBitmask<T1>::type;
+        using Integral = typename std::underlying_type<T>::type;
+        return ~static_cast<Integral>(LowerBitmask<T1>::Lower(t));
+    }
+
+    template <typename T,
+              typename T2,
+              typename = typename std::enable_if<IsDawnBitmask<T>::enable &&
+                                                 LowerBitmask<T2>::enable>::type>
+    constexpr T& operator&=(T& l, T2 right) {
+        T r = LowerBitmask<T2>::Lower(right);
+        l = l & r;
+        return l;
+    }
+
+    template <typename T,
+              typename T2,
+              typename = typename std::enable_if<IsDawnBitmask<T>::enable &&
+                                                 LowerBitmask<T2>::enable>::type>
+    constexpr T& operator|=(T& l, T2 right) {
+        T r = LowerBitmask<T2>::Lower(right);
+        l = l | r;
+        return l;
+    }
+
+    template <typename T,
+              typename T2,
+              typename = typename std::enable_if<IsDawnBitmask<T>::enable &&
+                                                 LowerBitmask<T2>::enable>::type>
+    constexpr T& operator^=(T& l, T2 right) {
+        T r = LowerBitmask<T2>::Lower(right);
+        l = l ^ r;
+        return l;
+    }
+
+    template <typename T>
+    constexpr bool HasZeroOrOneBits(T value) {
+        using Integral = typename std::underlying_type<T>::type;
+        return (static_cast<Integral>(value) & (static_cast<Integral>(value) - 1)) == 0;
+    }
+
+}  // namespace webnn
+
+#endif  // WEBNN_ENUM_CLASS_BITMASKS_H_
diff --git a/system/include/webnn/webnn.h b/system/include/webnn/webnn.h
new file mode 100644
index 000000000..67dd9241f
--- /dev/null
+++ b/system/include/webnn/webnn.h
@@ -0,0 +1,380 @@
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef WEBNN_H_
+#define WEBNN_H_
+
+#if defined(WEBNN_SHARED_LIBRARY)
+#    if defined(_WIN32)
+#        if defined(WEBNN_IMPLEMENTATION)
+#            define WEBNN_EXPORT __declspec(dllexport)
+#        else
+#            define WEBNN_EXPORT __declspec(dllimport)
+#        endif
+#    else  // defined(_WIN32)
+#        if defined(WEBNN_IMPLEMENTATION)
+#            define WEBNN_EXPORT __attribute__((visibility("default")))
+#        else
+#            define WEBNN_EXPORT
+#        endif
+#    endif  // defined(_WIN32)
+#else       // defined(WEBNN_SHARED_LIBRARY)
+#    define WEBNN_EXPORT
+#endif  // defined(WEBNN_SHARED_LIBRARY)
+
+#include <stdint.h>
+#include <stddef.h>
+#include <stdbool.h>
+
+typedef uint32_t WEBNNFlags;
+
+typedef struct MLContextImpl* MLContext;
+typedef struct MLGraphImpl* MLGraph;
+typedef struct MLGraphBuilderImpl* MLGraphBuilder;
+typedef struct MLNamedInputsImpl* MLNamedInputs;
+typedef struct MLNamedOperandsImpl* MLNamedOperands;
+typedef struct MLNamedOutputsImpl* MLNamedOutputs;
+typedef struct MLNamedResultsImpl* MLNamedResults;
+typedef struct MLOperandImpl* MLOperand;
+typedef struct MLResultImpl* MLResult;
+
+typedef enum MLAutoPad {
+    MLAutoPad_Explicit = 0x00000000,
+    MLAutoPad_SameUpper = 0x00000001,
+    MLAutoPad_SameLower = 0x00000002,
+    MLAutoPad_Force32 = 0x7FFFFFFF
+} MLAutoPad;
+
+typedef enum MLBuildGraphStatus {
+    MLBuildGraphStatus_Success = 0x00000000,
+    MLBuildGraphStatus_Error = 0x00000001,
+    MLBuildGraphStatus_ContextLost = 0x00000002,
+    MLBuildGraphStatus_Unknown = 0x00000003,
+    MLBuildGraphStatus_Force32 = 0x7FFFFFFF
+} MLBuildGraphStatus;
+
+typedef enum MLComputeGraphStatus {
+    MLComputeGraphStatus_Success = 0x00000000,
+    MLComputeGraphStatus_Error = 0x00000001,
+    MLComputeGraphStatus_ContextLost = 0x00000002,
+    MLComputeGraphStatus_Unknown = 0x00000003,
+    MLComputeGraphStatus_Force32 = 0x7FFFFFFF
+} MLComputeGraphStatus;
+
+typedef enum MLErrorFilter {
+    MLErrorFilter_None = 0x00000000,
+    MLErrorFilter_Validation = 0x00000001,
+    MLErrorFilter_OutOfMemory = 0x00000002,
+    MLErrorFilter_Force32 = 0x7FFFFFFF
+} MLErrorFilter;
+
+typedef enum MLErrorType {
+    MLErrorType_NoError = 0x00000000,
+    MLErrorType_Validation = 0x00000001,
+    MLErrorType_OutOfMemory = 0x00000002,
+    MLErrorType_Unknown = 0x00000003,
+    MLErrorType_DeviceLost = 0x00000004,
+    MLErrorType_Force32 = 0x7FFFFFFF
+} MLErrorType;
+
+typedef enum MLFilterOperandLayout {
+    MLFilterOperandLayout_Oihw = 0x00000000,
+    MLFilterOperandLayout_Hwio = 0x00000001,
+    MLFilterOperandLayout_Ohwi = 0x00000002,
+    MLFilterOperandLayout_Ihwo = 0x00000003,
+    MLFilterOperandLayout_Force32 = 0x7FFFFFFF
+} MLFilterOperandLayout;
+
+typedef enum MLFusedActivation {
+    MLFusedActivation_None = 0x00000000,
+    MLFusedActivation_Relu = 0x00000001,
+    MLFusedActivation_Force32 = 0x7FFFFFFF
+} MLFusedActivation;
+
+typedef enum MLInputOperandLayout {
+    MLInputOperandLayout_Nchw = 0x00000000,
+    MLInputOperandLayout_Nhwc = 0x00000001,
+    MLInputOperandLayout_Force32 = 0x7FFFFFFF
+} MLInputOperandLayout;
+
+typedef enum MLOperandType {
+    MLOperandType_Float32 = 0x00000000,
+    MLOperandType_Float16 = 0x00000001,
+    MLOperandType_Int32 = 0x00000002,
+    MLOperandType_Uint32 = 0x00000003,
+    MLOperandType_Int8 = 0x00000004,
+    MLOperandType_Uint8 = 0x00000005,
+    MLOperandType_Force32 = 0x7FFFFFFF
+} MLOperandType;
+
+typedef enum MLPowerPreference {
+    MLPowerPreference_Default = 0x00000000,
+    MLPowerPreference_High_performance = 0x00000001,
+    MLPowerPreference_Low_power = 0x00000002,
+    MLPowerPreference_Force32 = 0x7FFFFFFF
+} MLPowerPreference;
+
+
+typedef struct MLBatchNormOptions {
+    MLOperand scale;
+    MLOperand bias;
+    uint32_t axis;
+    float epsilon;
+    MLFusedActivation activation;
+} MLBatchNormOptions;
+
+typedef struct MLClampOptions {
+    MLOperand minValue;
+    MLOperand maxValue;
+} MLClampOptions;
+
+typedef struct MLContextOptions {
+    MLPowerPreference powerPreference;
+} MLContextOptions;
+
+typedef struct MLConv2dOptions {
+    uint32_t paddingCount;
+    int32_t const * padding;
+    uint32_t stridesCount;
+    int32_t const * strides;
+    uint32_t dilationsCount;
+    int32_t const * dilations;
+    MLAutoPad autoPad;
+    int32_t groups;
+    MLInputOperandLayout inputLayout;
+    MLFilterOperandLayout filterLayout;
+    MLOperand bias;
+    MLFusedActivation activation;
+} MLConv2dOptions;
+
+typedef struct MLGemmOptions {
+    MLOperand c;
+    float alpha;
+    float beta;
+    bool aTranspose;
+    bool bTranspose;
+} MLGemmOptions;
+
+typedef struct MLInput {
+    void const * buffer;
+    size_t size;
+    int32_t const * dimensions;
+    uint32_t dimensionsCount;
+} MLInput;
+
+typedef struct MLLeakyReluOptions {
+    float alpha;
+} MLLeakyReluOptions;
+
+typedef struct MLOperandDescriptor {
+    MLOperandType type;
+    int32_t const * dimensions;
+    uint32_t dimensionsCount;
+} MLOperandDescriptor;
+
+typedef struct MLOutput {
+    void * buffer;
+    size_t size;
+    int32_t const * dimensions;
+    uint32_t dimensionsCount;
+} MLOutput;
+
+typedef struct MLPool2dOptions {
+    uint32_t windowDimensionsCount;
+    int32_t const * windowDimensions;
+    uint32_t paddingCount;
+    int32_t const * padding;
+    uint32_t stridesCount;
+    int32_t const * strides;
+    uint32_t dilationsCount;
+    int32_t const * dilations;
+    MLAutoPad autoPad;
+    MLInputOperandLayout layout;
+} MLPool2dOptions;
+
+typedef struct MLTransposeOptions {
+    uint32_t permutationCount;
+    int32_t const * permutation;
+} MLTransposeOptions;
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+typedef void (*MLBuildGraphCallback)(MLBuildGraphStatus status, MLGraph graph, char const * message, void * userdata);
+typedef void (*MLComputeGraphCallback)(MLComputeGraphStatus status, MLNamedResults outputs, char const * message, void * userdata);
+typedef void (*MLErrorCallback)(MLErrorType type, char const * message, void * userdata);
+
+typedef void (*WebnnProc)(void);
+
+#if !defined(WEBNN_SKIP_PROCS)
+
+typedef MLGraphBuilder (*WebnnProcCreateGraphBuilder)(MLContext context);
+typedef MLNamedInputs (*WebnnProcCreateNamedInputs)();
+typedef MLNamedOperands (*WebnnProcCreateNamedOperands)();
+typedef MLNamedOutputs (*WebnnProcCreateNamedOutputs)();
+
+// Procs of Context
+typedef bool (*WebnnProcContextPopErrorScope)(MLContext context, MLErrorCallback callback, void * userdata);
+typedef void (*WebnnProcContextPushErrorScope)(MLContext context, MLErrorFilter filter);
+typedef void (*WebnnProcContextSetUncapturedErrorCallback)(MLContext context, MLErrorCallback callback, void * userdata);
+typedef void (*WebnnProcContextReference)(MLContext context);
+typedef void (*WebnnProcContextRelease)(MLContext context);
+
+// Procs of Graph
+typedef void (*WebnnProcGraphCompute)(MLGraph graph, MLNamedInputs inputs, MLComputeGraphCallback callback, void * userdata, MLNamedOutputs outputs);
+typedef MLComputeGraphStatus (*WebnnProcGraphComputeSync)(MLGraph graph, MLNamedInputs inputs, MLNamedOutputs outputs);
+typedef void (*WebnnProcGraphReference)(MLGraph graph);
+typedef void (*WebnnProcGraphRelease)(MLGraph graph);
+
+// Procs of GraphBuilder
+typedef MLOperand (*WebnnProcGraphBuilderAdd)(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+typedef MLOperand (*WebnnProcGraphBuilderAveragePool2d)(MLGraphBuilder graphBuilder, MLOperand input, MLPool2dOptions const * options);
+typedef MLOperand (*WebnnProcGraphBuilderBatchNorm)(MLGraphBuilder graphBuilder, MLOperand input, MLOperand mean, MLOperand variance, MLBatchNormOptions const * options);
+typedef void (*WebnnProcGraphBuilderBuild)(MLGraphBuilder graphBuilder, MLNamedOperands namedOperands, MLBuildGraphCallback callback, void * userdata);
+typedef MLGraph (*WebnnProcGraphBuilderBuildSync)(MLGraphBuilder graphBuilder, MLNamedOperands namedOperands);
+typedef MLOperand (*WebnnProcGraphBuilderClamp)(MLGraphBuilder graphBuilder, MLOperand input, MLClampOptions const * options);
+typedef MLOperand (*WebnnProcGraphBuilderConcat)(MLGraphBuilder graphBuilder, uint32_t inputsCount, MLOperand const * inputs, uint32_t axis);
+typedef MLOperand (*WebnnProcGraphBuilderConstant)(MLGraphBuilder graphBuilder, MLOperandDescriptor const * desc, void const * value, size_t size);
+typedef MLOperand (*WebnnProcGraphBuilderConv2d)(MLGraphBuilder graphBuilder, MLOperand input, MLOperand filter, MLConv2dOptions const * options);
+typedef MLOperand (*WebnnProcGraphBuilderGemm)(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b, MLGemmOptions const * options);
+typedef MLOperand (*WebnnProcGraphBuilderInput)(MLGraphBuilder graphBuilder, char const * name, MLOperandDescriptor const * desc);
+typedef MLOperand (*WebnnProcGraphBuilderLeakyRelu)(MLGraphBuilder graphBuilder, MLOperand input, MLLeakyReluOptions const * options);
+typedef MLOperand (*WebnnProcGraphBuilderMatmul)(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+typedef MLOperand (*WebnnProcGraphBuilderMaxPool2d)(MLGraphBuilder graphBuilder, MLOperand input, MLPool2dOptions const * options);
+typedef MLOperand (*WebnnProcGraphBuilderMul)(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+typedef MLOperand (*WebnnProcGraphBuilderRelu)(MLGraphBuilder graphBuilder, MLOperand input);
+typedef MLOperand (*WebnnProcGraphBuilderReshape)(MLGraphBuilder graphBuilder, MLOperand input, int32_t const * newShape, uint32_t newShapeCount);
+typedef MLOperand (*WebnnProcGraphBuilderSoftmax)(MLGraphBuilder graphBuilder, MLOperand input);
+typedef MLOperand (*WebnnProcGraphBuilderSub)(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+typedef MLOperand (*WebnnProcGraphBuilderTranspose)(MLGraphBuilder graphBuilder, MLOperand input, MLTransposeOptions const * options);
+typedef void (*WebnnProcGraphBuilderReference)(MLGraphBuilder graphBuilder);
+typedef void (*WebnnProcGraphBuilderRelease)(MLGraphBuilder graphBuilder);
+
+// Procs of NamedInputs
+typedef void (*WebnnProcNamedInputsSet)(MLNamedInputs namedInputs, char const * name, MLInput const * input);
+typedef void (*WebnnProcNamedInputsReference)(MLNamedInputs namedInputs);
+typedef void (*WebnnProcNamedInputsRelease)(MLNamedInputs namedInputs);
+
+// Procs of NamedOperands
+typedef void (*WebnnProcNamedOperandsSet)(MLNamedOperands namedOperands, char const * name, MLOperand operand);
+typedef void (*WebnnProcNamedOperandsReference)(MLNamedOperands namedOperands);
+typedef void (*WebnnProcNamedOperandsRelease)(MLNamedOperands namedOperands);
+
+// Procs of NamedOutputs
+typedef void (*WebnnProcNamedOutputsSet)(MLNamedOutputs namedOutputs, char const * name, MLOutput const * output);
+typedef void (*WebnnProcNamedOutputsReference)(MLNamedOutputs namedOutputs);
+typedef void (*WebnnProcNamedOutputsRelease)(MLNamedOutputs namedOutputs);
+
+// Procs of NamedResults
+typedef MLResult (*WebnnProcNamedResultsGet)(MLNamedResults namedResults, char const * name);
+typedef void (*WebnnProcNamedResultsReference)(MLNamedResults namedResults);
+typedef void (*WebnnProcNamedResultsRelease)(MLNamedResults namedResults);
+
+// Procs of Operand
+typedef void (*WebnnProcOperandReference)(MLOperand operand);
+typedef void (*WebnnProcOperandRelease)(MLOperand operand);
+
+// Procs of Result
+typedef const void* (*WebnnProcResultBuffer)(MLResult result);
+typedef uint32_t (*WebnnProcResultBufferSize)(MLResult result);
+typedef const int32_t* (*WebnnProcResultDimensions)(MLResult result);
+typedef uint32_t (*WebnnProcResultDimensionsSize)(MLResult result);
+typedef void (*WebnnProcResultReference)(MLResult result);
+typedef void (*WebnnProcResultRelease)(MLResult result);
+
+#endif  // !defined(WEBNN_SKIP_PROCS)
+
+#if !defined(WEBNN_SKIP_DECLARATIONS)
+
+WEBNN_EXPORT MLGraphBuilder webnnCreateGraphBuilder(MLContext context);
+WEBNN_EXPORT MLNamedInputs webnnCreateNamedInputs();
+WEBNN_EXPORT MLNamedOperands webnnCreateNamedOperands();
+WEBNN_EXPORT MLNamedOutputs webnnCreateNamedOutputs();
+
+// Methods of Context
+WEBNN_EXPORT bool mlContextPopErrorScope(MLContext context, MLErrorCallback callback, void * userdata);
+WEBNN_EXPORT void mlContextPushErrorScope(MLContext context, MLErrorFilter filter);
+WEBNN_EXPORT void mlContextSetUncapturedErrorCallback(MLContext context, MLErrorCallback callback, void * userdata);
+WEBNN_EXPORT void mlContextReference(MLContext context);
+WEBNN_EXPORT void mlContextRelease(MLContext context);
+
+// Methods of Graph
+WEBNN_EXPORT void mlGraphCompute(MLGraph graph, MLNamedInputs inputs, MLComputeGraphCallback callback, void * userdata, MLNamedOutputs outputs);
+WEBNN_EXPORT MLComputeGraphStatus mlGraphComputeSync(MLGraph graph, MLNamedInputs inputs, MLNamedOutputs outputs);
+WEBNN_EXPORT void mlGraphReference(MLGraph graph);
+WEBNN_EXPORT void mlGraphRelease(MLGraph graph);
+
+// Methods of GraphBuilder
+WEBNN_EXPORT MLOperand mlGraphBuilderAdd(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+WEBNN_EXPORT MLOperand mlGraphBuilderAveragePool2d(MLGraphBuilder graphBuilder, MLOperand input, MLPool2dOptions const * options);
+WEBNN_EXPORT MLOperand mlGraphBuilderBatchNorm(MLGraphBuilder graphBuilder, MLOperand input, MLOperand mean, MLOperand variance, MLBatchNormOptions const * options);
+WEBNN_EXPORT void mlGraphBuilderBuild(MLGraphBuilder graphBuilder, MLNamedOperands namedOperands, MLBuildGraphCallback callback, void * userdata);
+WEBNN_EXPORT MLGraph mlGraphBuilderBuildSync(MLGraphBuilder graphBuilder, MLNamedOperands namedOperands);
+WEBNN_EXPORT MLOperand mlGraphBuilderClamp(MLGraphBuilder graphBuilder, MLOperand input, MLClampOptions const * options);
+WEBNN_EXPORT MLOperand mlGraphBuilderConcat(MLGraphBuilder graphBuilder, uint32_t inputsCount, MLOperand const * inputs, uint32_t axis);
+WEBNN_EXPORT MLOperand mlGraphBuilderConstant(MLGraphBuilder graphBuilder, MLOperandDescriptor const * desc, void const * value, size_t size);
+WEBNN_EXPORT MLOperand mlGraphBuilderConv2d(MLGraphBuilder graphBuilder, MLOperand input, MLOperand filter, MLConv2dOptions const * options);
+WEBNN_EXPORT MLOperand mlGraphBuilderGemm(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b, MLGemmOptions const * options);
+WEBNN_EXPORT MLOperand mlGraphBuilderInput(MLGraphBuilder graphBuilder, char const * name, MLOperandDescriptor const * desc);
+WEBNN_EXPORT MLOperand mlGraphBuilderLeakyRelu(MLGraphBuilder graphBuilder, MLOperand input, MLLeakyReluOptions const * options);
+WEBNN_EXPORT MLOperand mlGraphBuilderMatmul(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+WEBNN_EXPORT MLOperand mlGraphBuilderMaxPool2d(MLGraphBuilder graphBuilder, MLOperand input, MLPool2dOptions const * options);
+WEBNN_EXPORT MLOperand mlGraphBuilderMul(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+WEBNN_EXPORT MLOperand mlGraphBuilderRelu(MLGraphBuilder graphBuilder, MLOperand input);
+WEBNN_EXPORT MLOperand mlGraphBuilderReshape(MLGraphBuilder graphBuilder, MLOperand input, int32_t const * newShape, uint32_t newShapeCount);
+WEBNN_EXPORT MLOperand mlGraphBuilderSoftmax(MLGraphBuilder graphBuilder, MLOperand input);
+WEBNN_EXPORT MLOperand mlGraphBuilderSub(MLGraphBuilder graphBuilder, MLOperand a, MLOperand b);
+WEBNN_EXPORT MLOperand mlGraphBuilderTranspose(MLGraphBuilder graphBuilder, MLOperand input, MLTransposeOptions const * options);
+WEBNN_EXPORT void mlGraphBuilderReference(MLGraphBuilder graphBuilder);
+WEBNN_EXPORT void mlGraphBuilderRelease(MLGraphBuilder graphBuilder);
+
+// Methods of NamedInputs
+WEBNN_EXPORT void mlNamedInputsSet(MLNamedInputs namedInputs, char const * name, MLInput const * input);
+WEBNN_EXPORT void mlNamedInputsReference(MLNamedInputs namedInputs);
+WEBNN_EXPORT void mlNamedInputsRelease(MLNamedInputs namedInputs);
+
+// Methods of NamedOperands
+WEBNN_EXPORT void mlNamedOperandsSet(MLNamedOperands namedOperands, char const * name, MLOperand operand);
+WEBNN_EXPORT void mlNamedOperandsReference(MLNamedOperands namedOperands);
+WEBNN_EXPORT void mlNamedOperandsRelease(MLNamedOperands namedOperands);
+
+// Methods of NamedOutputs
+WEBNN_EXPORT void mlNamedOutputsSet(MLNamedOutputs namedOutputs, char const * name, MLOutput const * output);
+WEBNN_EXPORT void mlNamedOutputsReference(MLNamedOutputs namedOutputs);
+WEBNN_EXPORT void mlNamedOutputsRelease(MLNamedOutputs namedOutputs);
+
+// Methods of NamedResults
+WEBNN_EXPORT MLResult mlNamedResultsGet(MLNamedResults namedResults, char const * name);
+WEBNN_EXPORT void mlNamedResultsReference(MLNamedResults namedResults);
+WEBNN_EXPORT void mlNamedResultsRelease(MLNamedResults namedResults);
+
+// Methods of Operand
+WEBNN_EXPORT void mlOperandReference(MLOperand operand);
+WEBNN_EXPORT void mlOperandRelease(MLOperand operand);
+
+// Methods of Result
+WEBNN_EXPORT const void* mlResultBuffer(MLResult result);
+WEBNN_EXPORT uint32_t mlResultBufferSize(MLResult result);
+WEBNN_EXPORT const int32_t* mlResultDimensions(MLResult result);
+WEBNN_EXPORT uint32_t mlResultDimensionsSize(MLResult result);
+WEBNN_EXPORT void mlResultReference(MLResult result);
+WEBNN_EXPORT void mlResultRelease(MLResult result);
+
+#endif  // !defined(WEBNN_SKIP_DECLARATIONS)
+
+#ifdef __cplusplus
+} // extern "C"
+#endif
+
+#endif // WEBNN_H_
diff --git a/system/include/webnn/webnn_cpp.h b/system/include/webnn/webnn_cpp.h
new file mode 100644
index 000000000..254bbe037
--- /dev/null
+++ b/system/include/webnn/webnn_cpp.h
@@ -0,0 +1,419 @@
+#ifndef WEBNN_CPP_H_
+#define WEBNN_CPP_H_
+
+#include "webnn/webnn.h"
+#include "webnn/EnumClassBitmasks.h"
+
+namespace ml {
+
+    enum class AutoPad : uint32_t {
+        Explicit = 0x00000000,
+        SameUpper = 0x00000001,
+        SameLower = 0x00000002,
+    };
+
+    enum class BuildGraphStatus : uint32_t {
+        Success = 0x00000000,
+        Error = 0x00000001,
+        ContextLost = 0x00000002,
+        Unknown = 0x00000003,
+    };
+
+    enum class ComputeGraphStatus : uint32_t {
+        Success = 0x00000000,
+        Error = 0x00000001,
+        ContextLost = 0x00000002,
+        Unknown = 0x00000003,
+    };
+
+    enum class ErrorFilter : uint32_t {
+        None = 0x00000000,
+        Validation = 0x00000001,
+        OutOfMemory = 0x00000002,
+    };
+
+    enum class ErrorType : uint32_t {
+        NoError = 0x00000000,
+        Validation = 0x00000001,
+        OutOfMemory = 0x00000002,
+        Unknown = 0x00000003,
+        DeviceLost = 0x00000004,
+    };
+
+    enum class FilterOperandLayout : uint32_t {
+        Oihw = 0x00000000,
+        Hwio = 0x00000001,
+        Ohwi = 0x00000002,
+        Ihwo = 0x00000003,
+    };
+
+    enum class FusedActivation : uint32_t {
+        None = 0x00000000,
+        Relu = 0x00000001,
+    };
+
+    enum class InputOperandLayout : uint32_t {
+        Nchw = 0x00000000,
+        Nhwc = 0x00000001,
+    };
+
+    enum class OperandType : uint32_t {
+        Float32 = 0x00000000,
+        Float16 = 0x00000001,
+        Int32 = 0x00000002,
+        Uint32 = 0x00000003,
+        Int8 = 0x00000004,
+        Uint8 = 0x00000005,
+    };
+
+    enum class PowerPreference : uint32_t {
+        Default = 0x00000000,
+        High_performance = 0x00000001,
+        Low_power = 0x00000002,
+    };
+
+
+
+
+    using Proc = WebnnProc;
+    using BuildGraphCallback = MLBuildGraphCallback;
+    using ComputeGraphCallback = MLComputeGraphCallback;
+    using ErrorCallback = MLErrorCallback;
+
+    class Context;
+    class Graph;
+    class GraphBuilder;
+    class NamedInputs;
+    class NamedOperands;
+    class NamedOutputs;
+    class NamedResults;
+    class Operand;
+    class Result;
+
+    struct BatchNormOptions;
+    struct ClampOptions;
+    struct ContextOptions;
+    struct Conv2dOptions;
+    struct GemmOptions;
+    struct Input;
+    struct LeakyReluOptions;
+    struct OperandDescriptor;
+    struct Output;
+    struct Pool2dOptions;
+    struct TransposeOptions;
+
+    template<typename Derived, typename CType>
+    class ObjectBase {
+      public:
+        ObjectBase() = default;
+        ObjectBase(CType handle): mHandle(handle) {
+            if (mHandle) Derived::WebnnReference(mHandle);
+        }
+        ~ObjectBase() {
+            if (mHandle) Derived::WebnnRelease(mHandle);
+        }
+
+        ObjectBase(ObjectBase const& other)
+            : ObjectBase(other.GetHandle()) {
+        }
+        Derived& operator=(ObjectBase const& other) {
+            if (&other != this) {
+                if (mHandle) Derived::WebnnRelease(mHandle);
+                mHandle = other.mHandle;
+                if (mHandle) Derived::WebnnReference(mHandle);
+            }
+
+            return static_cast<Derived&>(*this);
+        }
+
+        ObjectBase(ObjectBase&& other) {
+            mHandle = other.mHandle;
+            other.mHandle = 0;
+        }
+        Derived& operator=(ObjectBase&& other) {
+            if (&other != this) {
+                if (mHandle) Derived::WebnnRelease(mHandle);
+                mHandle = other.mHandle;
+                other.mHandle = 0;
+            }
+
+            return static_cast<Derived&>(*this);
+        }
+
+        ObjectBase(std::nullptr_t) {}
+        Derived& operator=(std::nullptr_t) {
+            if (mHandle != nullptr) {
+                Derived::WebnnRelease(mHandle);
+                mHandle = nullptr;
+            }
+            return static_cast<Derived&>(*this);
+        }
+
+        bool operator==(std::nullptr_t) const {
+            return mHandle == nullptr;
+        }
+        bool operator!=(std::nullptr_t) const {
+            return mHandle != nullptr;
+        }
+
+        explicit operator bool() const {
+            return mHandle != nullptr;
+        }
+        CType GetHandle() const {
+            return mHandle;
+        }
+        CType Release() {
+            CType result = mHandle;
+            mHandle = 0;
+            return result;
+        }
+        static Derived Acquire(CType handle) {
+            Derived result;
+            result.mHandle = handle;
+            return result;
+        }
+
+      protected:
+        CType mHandle = nullptr;
+    };
+
+
+
+    class Context : public ObjectBase<Context, MLContext> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        bool PopErrorScope(ErrorCallback callback, void * userdata) const;
+        void PushErrorScope(ErrorFilter filter) const;
+        void SetUncapturedErrorCallback(ErrorCallback callback, void * userdata) const;
+
+      private:
+        friend ObjectBase<Context, MLContext>;
+        static void WebnnReference(MLContext handle);
+        static void WebnnRelease(MLContext handle);
+    };
+
+    class Graph : public ObjectBase<Graph, MLGraph> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        void Compute(NamedInputs const& inputs, ComputeGraphCallback callback, void * userdata, NamedOutputs const& outputs) const;
+        ComputeGraphStatus ComputeSync(NamedInputs const& inputs, NamedOutputs const& outputs) const;
+
+      private:
+        friend ObjectBase<Graph, MLGraph>;
+        static void WebnnReference(MLGraph handle);
+        static void WebnnRelease(MLGraph handle);
+    };
+
+    class GraphBuilder : public ObjectBase<GraphBuilder, MLGraphBuilder> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        Operand Add(Operand const& a, Operand const& b) const;
+        Operand AveragePool2d(Operand const& input, Pool2dOptions const * options = nullptr) const;
+        Operand BatchNorm(Operand const& input, Operand const& mean, Operand const& variance, BatchNormOptions const * options = nullptr) const;
+        void Build(NamedOperands const& namedOperands, BuildGraphCallback callback, void * userdata) const;
+        Graph BuildSync(NamedOperands const& namedOperands) const;
+        Operand Clamp(Operand const& input, ClampOptions const * options = nullptr) const;
+        Operand Concat(uint32_t inputsCount, Operand const * inputs, uint32_t axis) const;
+        Operand Constant(OperandDescriptor const * desc, void const * value, size_t size) const;
+        Operand Conv2d(Operand const& input, Operand const& filter, Conv2dOptions const * options = nullptr) const;
+        Operand Gemm(Operand const& a, Operand const& b, GemmOptions const * options = nullptr) const;
+        Operand Input(char const * name, OperandDescriptor const * desc) const;
+        Operand LeakyRelu(Operand const& input, LeakyReluOptions const * options = nullptr) const;
+        Operand Matmul(Operand const& a, Operand const& b) const;
+        Operand MaxPool2d(Operand const& input, Pool2dOptions const * options = nullptr) const;
+        Operand Mul(Operand const& a, Operand const& b) const;
+        Operand Relu(Operand const& input) const;
+        Operand Reshape(Operand const& input, int32_t const * newShape, uint32_t newShapeCount) const;
+        Operand Softmax(Operand const& input) const;
+        Operand Sub(Operand const& a, Operand const& b) const;
+        Operand Transpose(Operand const& input, TransposeOptions const * options = nullptr) const;
+
+      private:
+        friend ObjectBase<GraphBuilder, MLGraphBuilder>;
+        static void WebnnReference(MLGraphBuilder handle);
+        static void WebnnRelease(MLGraphBuilder handle);
+    };
+
+    class NamedInputs : public ObjectBase<NamedInputs, MLNamedInputs> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        void Set(char const * name, Input const * input) const;
+
+      private:
+        friend ObjectBase<NamedInputs, MLNamedInputs>;
+        static void WebnnReference(MLNamedInputs handle);
+        static void WebnnRelease(MLNamedInputs handle);
+    };
+
+    class NamedOperands : public ObjectBase<NamedOperands, MLNamedOperands> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        void Set(char const * name, Operand const& operand) const;
+
+      private:
+        friend ObjectBase<NamedOperands, MLNamedOperands>;
+        static void WebnnReference(MLNamedOperands handle);
+        static void WebnnRelease(MLNamedOperands handle);
+    };
+
+    class NamedOutputs : public ObjectBase<NamedOutputs, MLNamedOutputs> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        void Set(char const * name, Output const * output) const;
+
+      private:
+        friend ObjectBase<NamedOutputs, MLNamedOutputs>;
+        static void WebnnReference(MLNamedOutputs handle);
+        static void WebnnRelease(MLNamedOutputs handle);
+    };
+
+    class NamedResults : public ObjectBase<NamedResults, MLNamedResults> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        Result Get(char const * name) const;
+
+      private:
+        friend ObjectBase<NamedResults, MLNamedResults>;
+        static void WebnnReference(MLNamedResults handle);
+        static void WebnnRelease(MLNamedResults handle);
+    };
+
+    class Operand : public ObjectBase<Operand, MLOperand> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+
+      private:
+        friend ObjectBase<Operand, MLOperand>;
+        static void WebnnReference(MLOperand handle);
+        static void WebnnRelease(MLOperand handle);
+    };
+
+    class Result : public ObjectBase<Result, MLResult> {
+      public:
+        using ObjectBase::ObjectBase;
+        using ObjectBase::operator=;
+
+        const void* Buffer() const;
+        uint32_t BufferSize() const;
+        const int32_t* Dimensions() const;
+        uint32_t DimensionsSize() const;
+
+      private:
+        friend ObjectBase<Result, MLResult>;
+        static void WebnnReference(MLResult handle);
+        static void WebnnRelease(MLResult handle);
+    };
+
+
+    struct ChainedStruct {
+        ChainedStruct const * nextInChain = nullptr;
+        // SType sType = SType::Invalid;
+    };
+
+    struct BatchNormOptions {
+        Operand scale;
+        Operand bias;
+        uint32_t axis = 1;
+        float epsilon = 1e-05;
+        FusedActivation activation = FusedActivation::None;
+    };
+
+    struct ClampOptions {
+        Operand minValue;
+        Operand maxValue;
+    };
+
+    struct ContextOptions {
+        PowerPreference powerPreference = PowerPreference::Default;
+    };
+
+    struct Conv2dOptions {
+        uint32_t paddingCount = 0;
+        int32_t const * padding = nullptr;
+        uint32_t stridesCount = 0;
+        int32_t const * strides = nullptr;
+        uint32_t dilationsCount = 0;
+        int32_t const * dilations = nullptr;
+        AutoPad autoPad = AutoPad::Explicit;
+        int32_t groups = 1;
+        InputOperandLayout inputLayout = InputOperandLayout::Nchw;
+        FilterOperandLayout filterLayout = FilterOperandLayout::Oihw;
+        Operand bias;
+        FusedActivation activation = FusedActivation::None;
+    };
+
+    struct GemmOptions {
+        Operand c;
+        float alpha = 1.0;
+        float beta = 1.0;
+        bool aTranspose = false;
+        bool bTranspose = false;
+    };
+
+    struct Input {
+        void const * buffer;
+        size_t size;
+        int32_t const * dimensions = nullptr;
+        uint32_t dimensionsCount = 0;
+    };
+
+    struct LeakyReluOptions {
+        float alpha = 0.01;
+    };
+
+    struct OperandDescriptor {
+        OperandType type;
+        int32_t const * dimensions;
+        uint32_t dimensionsCount = 0;
+    };
+
+    struct Output {
+        void * buffer = nullptr;
+        size_t size;
+        int32_t const * dimensions = nullptr;
+        uint32_t dimensionsCount = 0;
+    };
+
+    struct Pool2dOptions {
+        uint32_t windowDimensionsCount = 0;
+        int32_t const * windowDimensions = nullptr;
+        uint32_t paddingCount = 0;
+        int32_t const * padding = nullptr;
+        uint32_t stridesCount = 0;
+        int32_t const * strides = nullptr;
+        uint32_t dilationsCount = 0;
+        int32_t const * dilations = nullptr;
+        AutoPad autoPad = AutoPad::Explicit;
+        InputOperandLayout layout = InputOperandLayout::Nchw;
+    };
+
+    struct TransposeOptions {
+        uint32_t permutationCount = 0;
+        int32_t const * permutation = nullptr;
+    };
+
+
+    GraphBuilder CreateGraphBuilder(Context context);
+    NamedInputs CreateNamedInputs();
+    NamedOperands CreateNamedOperands();
+    NamedOutputs CreateNamedOutputs();
+
+}  // namespace webnn
+
+#endif // WEBNN_CPP_H_
diff --git a/system/lib/webnn/webnn_cpp.cpp b/system/lib/webnn/webnn_cpp.cpp
new file mode 100644
index 000000000..7e1634446
--- /dev/null
+++ b/system/lib/webnn/webnn_cpp.cpp
@@ -0,0 +1,553 @@
+#include "webnn/webnn_cpp.h"
+
+namespace ml {
+
+    // AutoPad
+
+    static_assert(sizeof(AutoPad) == sizeof(MLAutoPad), "sizeof mismatch for AutoPad");
+    static_assert(alignof(AutoPad) == alignof(MLAutoPad), "alignof mismatch for AutoPad");
+
+    static_assert(static_cast<uint32_t>(AutoPad::Explicit) == MLAutoPad_Explicit, "value mismatch for AutoPad::Explicit");
+    static_assert(static_cast<uint32_t>(AutoPad::SameUpper) == MLAutoPad_SameUpper, "value mismatch for AutoPad::SameUpper");
+    static_assert(static_cast<uint32_t>(AutoPad::SameLower) == MLAutoPad_SameLower, "value mismatch for AutoPad::SameLower");
+
+    // BuildGraphStatus
+
+    static_assert(sizeof(BuildGraphStatus) == sizeof(MLBuildGraphStatus), "sizeof mismatch for BuildGraphStatus");
+    static_assert(alignof(BuildGraphStatus) == alignof(MLBuildGraphStatus), "alignof mismatch for BuildGraphStatus");
+
+    static_assert(static_cast<uint32_t>(BuildGraphStatus::Success) == MLBuildGraphStatus_Success, "value mismatch for BuildGraphStatus::Success");
+    static_assert(static_cast<uint32_t>(BuildGraphStatus::Error) == MLBuildGraphStatus_Error, "value mismatch for BuildGraphStatus::Error");
+    static_assert(static_cast<uint32_t>(BuildGraphStatus::ContextLost) == MLBuildGraphStatus_ContextLost, "value mismatch for BuildGraphStatus::ContextLost");
+    static_assert(static_cast<uint32_t>(BuildGraphStatus::Unknown) == MLBuildGraphStatus_Unknown, "value mismatch for BuildGraphStatus::Unknown");
+
+    // ComputeGraphStatus
+
+    static_assert(sizeof(ComputeGraphStatus) == sizeof(MLComputeGraphStatus), "sizeof mismatch for ComputeGraphStatus");
+    static_assert(alignof(ComputeGraphStatus) == alignof(MLComputeGraphStatus), "alignof mismatch for ComputeGraphStatus");
+
+    static_assert(static_cast<uint32_t>(ComputeGraphStatus::Success) == MLComputeGraphStatus_Success, "value mismatch for ComputeGraphStatus::Success");
+    static_assert(static_cast<uint32_t>(ComputeGraphStatus::Error) == MLComputeGraphStatus_Error, "value mismatch for ComputeGraphStatus::Error");
+    static_assert(static_cast<uint32_t>(ComputeGraphStatus::ContextLost) == MLComputeGraphStatus_ContextLost, "value mismatch for ComputeGraphStatus::ContextLost");
+    static_assert(static_cast<uint32_t>(ComputeGraphStatus::Unknown) == MLComputeGraphStatus_Unknown, "value mismatch for ComputeGraphStatus::Unknown");
+
+    // ErrorFilter
+
+    static_assert(sizeof(ErrorFilter) == sizeof(MLErrorFilter), "sizeof mismatch for ErrorFilter");
+    static_assert(alignof(ErrorFilter) == alignof(MLErrorFilter), "alignof mismatch for ErrorFilter");
+
+    static_assert(static_cast<uint32_t>(ErrorFilter::None) == MLErrorFilter_None, "value mismatch for ErrorFilter::None");
+    static_assert(static_cast<uint32_t>(ErrorFilter::Validation) == MLErrorFilter_Validation, "value mismatch for ErrorFilter::Validation");
+    static_assert(static_cast<uint32_t>(ErrorFilter::OutOfMemory) == MLErrorFilter_OutOfMemory, "value mismatch for ErrorFilter::OutOfMemory");
+
+    // ErrorType
+
+    static_assert(sizeof(ErrorType) == sizeof(MLErrorType), "sizeof mismatch for ErrorType");
+    static_assert(alignof(ErrorType) == alignof(MLErrorType), "alignof mismatch for ErrorType");
+
+    static_assert(static_cast<uint32_t>(ErrorType::NoError) == MLErrorType_NoError, "value mismatch for ErrorType::NoError");
+    static_assert(static_cast<uint32_t>(ErrorType::Validation) == MLErrorType_Validation, "value mismatch for ErrorType::Validation");
+    static_assert(static_cast<uint32_t>(ErrorType::OutOfMemory) == MLErrorType_OutOfMemory, "value mismatch for ErrorType::OutOfMemory");
+    static_assert(static_cast<uint32_t>(ErrorType::Unknown) == MLErrorType_Unknown, "value mismatch for ErrorType::Unknown");
+    static_assert(static_cast<uint32_t>(ErrorType::DeviceLost) == MLErrorType_DeviceLost, "value mismatch for ErrorType::DeviceLost");
+
+    // FilterOperandLayout
+
+    static_assert(sizeof(FilterOperandLayout) == sizeof(MLFilterOperandLayout), "sizeof mismatch for FilterOperandLayout");
+    static_assert(alignof(FilterOperandLayout) == alignof(MLFilterOperandLayout), "alignof mismatch for FilterOperandLayout");
+
+    static_assert(static_cast<uint32_t>(FilterOperandLayout::Oihw) == MLFilterOperandLayout_Oihw, "value mismatch for FilterOperandLayout::Oihw");
+    static_assert(static_cast<uint32_t>(FilterOperandLayout::Hwio) == MLFilterOperandLayout_Hwio, "value mismatch for FilterOperandLayout::Hwio");
+    static_assert(static_cast<uint32_t>(FilterOperandLayout::Ohwi) == MLFilterOperandLayout_Ohwi, "value mismatch for FilterOperandLayout::Ohwi");
+    static_assert(static_cast<uint32_t>(FilterOperandLayout::Ihwo) == MLFilterOperandLayout_Ihwo, "value mismatch for FilterOperandLayout::Ihwo");
+
+    // FusedActivation
+
+    static_assert(sizeof(FusedActivation) == sizeof(MLFusedActivation), "sizeof mismatch for FusedActivation");
+    static_assert(alignof(FusedActivation) == alignof(MLFusedActivation), "alignof mismatch for FusedActivation");
+
+    static_assert(static_cast<uint32_t>(FusedActivation::None) == MLFusedActivation_None, "value mismatch for FusedActivation::None");
+    static_assert(static_cast<uint32_t>(FusedActivation::Relu) == MLFusedActivation_Relu, "value mismatch for FusedActivation::Relu");
+
+    // InputOperandLayout
+
+    static_assert(sizeof(InputOperandLayout) == sizeof(MLInputOperandLayout), "sizeof mismatch for InputOperandLayout");
+    static_assert(alignof(InputOperandLayout) == alignof(MLInputOperandLayout), "alignof mismatch for InputOperandLayout");
+
+    static_assert(static_cast<uint32_t>(InputOperandLayout::Nchw) == MLInputOperandLayout_Nchw, "value mismatch for InputOperandLayout::Nchw");
+    static_assert(static_cast<uint32_t>(InputOperandLayout::Nhwc) == MLInputOperandLayout_Nhwc, "value mismatch for InputOperandLayout::Nhwc");
+
+    // OperandType
+
+    static_assert(sizeof(OperandType) == sizeof(MLOperandType), "sizeof mismatch for OperandType");
+    static_assert(alignof(OperandType) == alignof(MLOperandType), "alignof mismatch for OperandType");
+
+    static_assert(static_cast<uint32_t>(OperandType::Float32) == MLOperandType_Float32, "value mismatch for OperandType::Float32");
+    static_assert(static_cast<uint32_t>(OperandType::Float16) == MLOperandType_Float16, "value mismatch for OperandType::Float16");
+    static_assert(static_cast<uint32_t>(OperandType::Int32) == MLOperandType_Int32, "value mismatch for OperandType::Int32");
+    static_assert(static_cast<uint32_t>(OperandType::Uint32) == MLOperandType_Uint32, "value mismatch for OperandType::Uint32");
+    static_assert(static_cast<uint32_t>(OperandType::Int8) == MLOperandType_Int8, "value mismatch for OperandType::Int8");
+    static_assert(static_cast<uint32_t>(OperandType::Uint8) == MLOperandType_Uint8, "value mismatch for OperandType::Uint8");
+
+    // PowerPreference
+
+    static_assert(sizeof(PowerPreference) == sizeof(MLPowerPreference), "sizeof mismatch for PowerPreference");
+    static_assert(alignof(PowerPreference) == alignof(MLPowerPreference), "alignof mismatch for PowerPreference");
+
+    static_assert(static_cast<uint32_t>(PowerPreference::Default) == MLPowerPreference_Default, "value mismatch for PowerPreference::Default");
+    static_assert(static_cast<uint32_t>(PowerPreference::High_performance) == MLPowerPreference_High_performance, "value mismatch for PowerPreference::High_performance");
+    static_assert(static_cast<uint32_t>(PowerPreference::Low_power) == MLPowerPreference_Low_power, "value mismatch for PowerPreference::Low_power");
+
+    // ChainedStruct
+
+
+    // BatchNormOptions
+
+    static_assert(sizeof(BatchNormOptions) == sizeof(MLBatchNormOptions), "sizeof mismatch for BatchNormOptions");
+    static_assert(alignof(BatchNormOptions) == alignof(MLBatchNormOptions), "alignof mismatch for BatchNormOptions");
+
+    static_assert(offsetof(BatchNormOptions, scale) == offsetof(MLBatchNormOptions, scale),
+            "offsetof mismatch for BatchNormOptions::scale");
+    static_assert(offsetof(BatchNormOptions, bias) == offsetof(MLBatchNormOptions, bias),
+            "offsetof mismatch for BatchNormOptions::bias");
+    static_assert(offsetof(BatchNormOptions, axis) == offsetof(MLBatchNormOptions, axis),
+            "offsetof mismatch for BatchNormOptions::axis");
+    static_assert(offsetof(BatchNormOptions, epsilon) == offsetof(MLBatchNormOptions, epsilon),
+            "offsetof mismatch for BatchNormOptions::epsilon");
+    static_assert(offsetof(BatchNormOptions, activation) == offsetof(MLBatchNormOptions, activation),
+            "offsetof mismatch for BatchNormOptions::activation");
+
+    // ClampOptions
+
+    static_assert(sizeof(ClampOptions) == sizeof(MLClampOptions), "sizeof mismatch for ClampOptions");
+    static_assert(alignof(ClampOptions) == alignof(MLClampOptions), "alignof mismatch for ClampOptions");
+
+    static_assert(offsetof(ClampOptions, minValue) == offsetof(MLClampOptions, minValue),
+            "offsetof mismatch for ClampOptions::minValue");
+    static_assert(offsetof(ClampOptions, maxValue) == offsetof(MLClampOptions, maxValue),
+            "offsetof mismatch for ClampOptions::maxValue");
+
+    // ContextOptions
+
+    static_assert(sizeof(ContextOptions) == sizeof(MLContextOptions), "sizeof mismatch for ContextOptions");
+    static_assert(alignof(ContextOptions) == alignof(MLContextOptions), "alignof mismatch for ContextOptions");
+
+    static_assert(offsetof(ContextOptions, powerPreference) == offsetof(MLContextOptions, powerPreference),
+            "offsetof mismatch for ContextOptions::powerPreference");
+
+    // Conv2dOptions
+
+    static_assert(sizeof(Conv2dOptions) == sizeof(MLConv2dOptions), "sizeof mismatch for Conv2dOptions");
+    static_assert(alignof(Conv2dOptions) == alignof(MLConv2dOptions), "alignof mismatch for Conv2dOptions");
+
+    static_assert(offsetof(Conv2dOptions, paddingCount) == offsetof(MLConv2dOptions, paddingCount),
+            "offsetof mismatch for Conv2dOptions::paddingCount");
+    static_assert(offsetof(Conv2dOptions, padding) == offsetof(MLConv2dOptions, padding),
+            "offsetof mismatch for Conv2dOptions::padding");
+    static_assert(offsetof(Conv2dOptions, stridesCount) == offsetof(MLConv2dOptions, stridesCount),
+            "offsetof mismatch for Conv2dOptions::stridesCount");
+    static_assert(offsetof(Conv2dOptions, strides) == offsetof(MLConv2dOptions, strides),
+            "offsetof mismatch for Conv2dOptions::strides");
+    static_assert(offsetof(Conv2dOptions, dilationsCount) == offsetof(MLConv2dOptions, dilationsCount),
+            "offsetof mismatch for Conv2dOptions::dilationsCount");
+    static_assert(offsetof(Conv2dOptions, dilations) == offsetof(MLConv2dOptions, dilations),
+            "offsetof mismatch for Conv2dOptions::dilations");
+    static_assert(offsetof(Conv2dOptions, autoPad) == offsetof(MLConv2dOptions, autoPad),
+            "offsetof mismatch for Conv2dOptions::autoPad");
+    static_assert(offsetof(Conv2dOptions, groups) == offsetof(MLConv2dOptions, groups),
+            "offsetof mismatch for Conv2dOptions::groups");
+    static_assert(offsetof(Conv2dOptions, inputLayout) == offsetof(MLConv2dOptions, inputLayout),
+            "offsetof mismatch for Conv2dOptions::inputLayout");
+    static_assert(offsetof(Conv2dOptions, filterLayout) == offsetof(MLConv2dOptions, filterLayout),
+            "offsetof mismatch for Conv2dOptions::filterLayout");
+    static_assert(offsetof(Conv2dOptions, bias) == offsetof(MLConv2dOptions, bias),
+            "offsetof mismatch for Conv2dOptions::bias");
+    static_assert(offsetof(Conv2dOptions, activation) == offsetof(MLConv2dOptions, activation),
+            "offsetof mismatch for Conv2dOptions::activation");
+
+    // GemmOptions
+
+    static_assert(sizeof(GemmOptions) == sizeof(MLGemmOptions), "sizeof mismatch for GemmOptions");
+    static_assert(alignof(GemmOptions) == alignof(MLGemmOptions), "alignof mismatch for GemmOptions");
+
+    static_assert(offsetof(GemmOptions, c) == offsetof(MLGemmOptions, c),
+            "offsetof mismatch for GemmOptions::c");
+    static_assert(offsetof(GemmOptions, alpha) == offsetof(MLGemmOptions, alpha),
+            "offsetof mismatch for GemmOptions::alpha");
+    static_assert(offsetof(GemmOptions, beta) == offsetof(MLGemmOptions, beta),
+            "offsetof mismatch for GemmOptions::beta");
+    static_assert(offsetof(GemmOptions, aTranspose) == offsetof(MLGemmOptions, aTranspose),
+            "offsetof mismatch for GemmOptions::aTranspose");
+    static_assert(offsetof(GemmOptions, bTranspose) == offsetof(MLGemmOptions, bTranspose),
+            "offsetof mismatch for GemmOptions::bTranspose");
+
+    // Input
+
+    static_assert(sizeof(Input) == sizeof(MLInput), "sizeof mismatch for Input");
+    static_assert(alignof(Input) == alignof(MLInput), "alignof mismatch for Input");
+
+    static_assert(offsetof(Input, buffer) == offsetof(MLInput, buffer),
+            "offsetof mismatch for Input::buffer");
+    static_assert(offsetof(Input, size) == offsetof(MLInput, size),
+            "offsetof mismatch for Input::size");
+    static_assert(offsetof(Input, dimensions) == offsetof(MLInput, dimensions),
+            "offsetof mismatch for Input::dimensions");
+    static_assert(offsetof(Input, dimensionsCount) == offsetof(MLInput, dimensionsCount),
+            "offsetof mismatch for Input::dimensionsCount");
+
+    // LeakyReluOptions
+
+    static_assert(sizeof(LeakyReluOptions) == sizeof(MLLeakyReluOptions), "sizeof mismatch for LeakyReluOptions");
+    static_assert(alignof(LeakyReluOptions) == alignof(MLLeakyReluOptions), "alignof mismatch for LeakyReluOptions");
+
+    static_assert(offsetof(LeakyReluOptions, alpha) == offsetof(MLLeakyReluOptions, alpha),
+            "offsetof mismatch for LeakyReluOptions::alpha");
+
+    // OperandDescriptor
+
+    static_assert(sizeof(OperandDescriptor) == sizeof(MLOperandDescriptor), "sizeof mismatch for OperandDescriptor");
+    static_assert(alignof(OperandDescriptor) == alignof(MLOperandDescriptor), "alignof mismatch for OperandDescriptor");
+
+    static_assert(offsetof(OperandDescriptor, type) == offsetof(MLOperandDescriptor, type),
+            "offsetof mismatch for OperandDescriptor::type");
+    static_assert(offsetof(OperandDescriptor, dimensions) == offsetof(MLOperandDescriptor, dimensions),
+            "offsetof mismatch for OperandDescriptor::dimensions");
+    static_assert(offsetof(OperandDescriptor, dimensionsCount) == offsetof(MLOperandDescriptor, dimensionsCount),
+            "offsetof mismatch for OperandDescriptor::dimensionsCount");
+
+    // Output
+
+    static_assert(sizeof(Output) == sizeof(MLOutput), "sizeof mismatch for Output");
+    static_assert(alignof(Output) == alignof(MLOutput), "alignof mismatch for Output");
+
+    static_assert(offsetof(Output, buffer) == offsetof(MLOutput, buffer),
+            "offsetof mismatch for Output::buffer");
+    static_assert(offsetof(Output, size) == offsetof(MLOutput, size),
+            "offsetof mismatch for Output::size");
+    static_assert(offsetof(Output, dimensions) == offsetof(MLOutput, dimensions),
+            "offsetof mismatch for Output::dimensions");
+    static_assert(offsetof(Output, dimensionsCount) == offsetof(MLOutput, dimensionsCount),
+            "offsetof mismatch for Output::dimensionsCount");
+
+    // Pool2dOptions
+
+    static_assert(sizeof(Pool2dOptions) == sizeof(MLPool2dOptions), "sizeof mismatch for Pool2dOptions");
+    static_assert(alignof(Pool2dOptions) == alignof(MLPool2dOptions), "alignof mismatch for Pool2dOptions");
+
+    static_assert(offsetof(Pool2dOptions, windowDimensionsCount) == offsetof(MLPool2dOptions, windowDimensionsCount),
+            "offsetof mismatch for Pool2dOptions::windowDimensionsCount");
+    static_assert(offsetof(Pool2dOptions, windowDimensions) == offsetof(MLPool2dOptions, windowDimensions),
+            "offsetof mismatch for Pool2dOptions::windowDimensions");
+    static_assert(offsetof(Pool2dOptions, paddingCount) == offsetof(MLPool2dOptions, paddingCount),
+            "offsetof mismatch for Pool2dOptions::paddingCount");
+    static_assert(offsetof(Pool2dOptions, padding) == offsetof(MLPool2dOptions, padding),
+            "offsetof mismatch for Pool2dOptions::padding");
+    static_assert(offsetof(Pool2dOptions, stridesCount) == offsetof(MLPool2dOptions, stridesCount),
+            "offsetof mismatch for Pool2dOptions::stridesCount");
+    static_assert(offsetof(Pool2dOptions, strides) == offsetof(MLPool2dOptions, strides),
+            "offsetof mismatch for Pool2dOptions::strides");
+    static_assert(offsetof(Pool2dOptions, dilationsCount) == offsetof(MLPool2dOptions, dilationsCount),
+            "offsetof mismatch for Pool2dOptions::dilationsCount");
+    static_assert(offsetof(Pool2dOptions, dilations) == offsetof(MLPool2dOptions, dilations),
+            "offsetof mismatch for Pool2dOptions::dilations");
+    static_assert(offsetof(Pool2dOptions, autoPad) == offsetof(MLPool2dOptions, autoPad),
+            "offsetof mismatch for Pool2dOptions::autoPad");
+    static_assert(offsetof(Pool2dOptions, layout) == offsetof(MLPool2dOptions, layout),
+            "offsetof mismatch for Pool2dOptions::layout");
+
+    // TransposeOptions
+
+    static_assert(sizeof(TransposeOptions) == sizeof(MLTransposeOptions), "sizeof mismatch for TransposeOptions");
+    static_assert(alignof(TransposeOptions) == alignof(MLTransposeOptions), "alignof mismatch for TransposeOptions");
+
+    static_assert(offsetof(TransposeOptions, permutationCount) == offsetof(MLTransposeOptions, permutationCount),
+            "offsetof mismatch for TransposeOptions::permutationCount");
+    static_assert(offsetof(TransposeOptions, permutation) == offsetof(MLTransposeOptions, permutation),
+            "offsetof mismatch for TransposeOptions::permutation");
+
+    // Context
+
+    static_assert(sizeof(Context) == sizeof(MLContext), "sizeof mismatch for Context");
+    static_assert(alignof(Context) == alignof(MLContext), "alignof mismatch for Context");
+
+    bool Context::PopErrorScope(ErrorCallback callback, void * userdata) const {
+        auto result = mlContextPopErrorScope(GetHandle(), callback, reinterpret_cast<void * >(userdata));
+        return result;
+    }
+    void Context::PushErrorScope(ErrorFilter filter) const {
+        mlContextPushErrorScope(GetHandle(), static_cast<MLErrorFilter>(filter));
+    }
+    void Context::SetUncapturedErrorCallback(ErrorCallback callback, void * userdata) const {
+        mlContextSetUncapturedErrorCallback(GetHandle(), callback, reinterpret_cast<void * >(userdata));
+    }
+    void Context::WebnnReference(MLContext handle) {
+        if (handle != nullptr) {
+            mlContextReference(handle);
+        }
+    }
+    void Context::WebnnRelease(MLContext handle) {
+        if (handle != nullptr) {
+            mlContextRelease(handle);
+        }
+    }
+
+    // Graph
+
+    static_assert(sizeof(Graph) == sizeof(MLGraph), "sizeof mismatch for Graph");
+    static_assert(alignof(Graph) == alignof(MLGraph), "alignof mismatch for Graph");
+
+    void Graph::Compute(NamedInputs const& inputs, ComputeGraphCallback callback, void * userdata, NamedOutputs const& outputs) const {
+        mlGraphCompute(GetHandle(), inputs.GetHandle(), callback, reinterpret_cast<void * >(userdata), outputs.GetHandle());
+    }
+    ComputeGraphStatus Graph::ComputeSync(NamedInputs const& inputs, NamedOutputs const& outputs) const {
+        auto result = mlGraphComputeSync(GetHandle(), inputs.GetHandle(), outputs.GetHandle());
+        return static_cast<ComputeGraphStatus>(result);
+    }
+    void Graph::WebnnReference(MLGraph handle) {
+        if (handle != nullptr) {
+            mlGraphReference(handle);
+        }
+    }
+    void Graph::WebnnRelease(MLGraph handle) {
+        if (handle != nullptr) {
+            mlGraphRelease(handle);
+        }
+    }
+
+    // GraphBuilder
+
+    static_assert(sizeof(GraphBuilder) == sizeof(MLGraphBuilder), "sizeof mismatch for GraphBuilder");
+    static_assert(alignof(GraphBuilder) == alignof(MLGraphBuilder), "alignof mismatch for GraphBuilder");
+
+    Operand GraphBuilder::Add(Operand const& a, Operand const& b) const {
+        auto result = mlGraphBuilderAdd(GetHandle(), a.GetHandle(), b.GetHandle());
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::AveragePool2d(Operand const& input, Pool2dOptions const * options) const {
+        auto result = mlGraphBuilderAveragePool2d(GetHandle(), input.GetHandle(), reinterpret_cast<MLPool2dOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::BatchNorm(Operand const& input, Operand const& mean, Operand const& variance, BatchNormOptions const * options) const {
+        auto result = mlGraphBuilderBatchNorm(GetHandle(), input.GetHandle(), mean.GetHandle(), variance.GetHandle(), reinterpret_cast<MLBatchNormOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    void GraphBuilder::Build(NamedOperands const& namedOperands, BuildGraphCallback callback, void * userdata) const {
+        mlGraphBuilderBuild(GetHandle(), namedOperands.GetHandle(), callback, reinterpret_cast<void * >(userdata));
+    }
+    Graph GraphBuilder::BuildSync(NamedOperands const& namedOperands) const {
+        auto result = mlGraphBuilderBuildSync(GetHandle(), namedOperands.GetHandle());
+        return Graph::Acquire(result);
+    }
+    Operand GraphBuilder::Clamp(Operand const& input, ClampOptions const * options) const {
+        auto result = mlGraphBuilderClamp(GetHandle(), input.GetHandle(), reinterpret_cast<MLClampOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Concat(uint32_t inputsCount, Operand const * inputs, uint32_t axis) const {
+        auto result = mlGraphBuilderConcat(GetHandle(), inputsCount, reinterpret_cast<MLOperand const * >(inputs), axis);
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Constant(OperandDescriptor const * desc, void const * value, size_t size) const {
+        auto result = mlGraphBuilderConstant(GetHandle(), reinterpret_cast<MLOperandDescriptor const * >(desc), reinterpret_cast<void const * >(value), size);
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Conv2d(Operand const& input, Operand const& filter, Conv2dOptions const * options) const {
+        auto result = mlGraphBuilderConv2d(GetHandle(), input.GetHandle(), filter.GetHandle(), reinterpret_cast<MLConv2dOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Gemm(Operand const& a, Operand const& b, GemmOptions const * options) const {
+        auto result = mlGraphBuilderGemm(GetHandle(), a.GetHandle(), b.GetHandle(), reinterpret_cast<MLGemmOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Input(char const * name, OperandDescriptor const * desc) const {
+        auto result = mlGraphBuilderInput(GetHandle(), reinterpret_cast<char const * >(name), reinterpret_cast<MLOperandDescriptor const * >(desc));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::LeakyRelu(Operand const& input, LeakyReluOptions const * options) const {
+        auto result = mlGraphBuilderLeakyRelu(GetHandle(), input.GetHandle(), reinterpret_cast<MLLeakyReluOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Matmul(Operand const& a, Operand const& b) const {
+        auto result = mlGraphBuilderMatmul(GetHandle(), a.GetHandle(), b.GetHandle());
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::MaxPool2d(Operand const& input, Pool2dOptions const * options) const {
+        auto result = mlGraphBuilderMaxPool2d(GetHandle(), input.GetHandle(), reinterpret_cast<MLPool2dOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Mul(Operand const& a, Operand const& b) const {
+        auto result = mlGraphBuilderMul(GetHandle(), a.GetHandle(), b.GetHandle());
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Relu(Operand const& input) const {
+        auto result = mlGraphBuilderRelu(GetHandle(), input.GetHandle());
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Reshape(Operand const& input, int32_t const * newShape, uint32_t newShapeCount) const {
+        auto result = mlGraphBuilderReshape(GetHandle(), input.GetHandle(), reinterpret_cast<int32_t const * >(newShape), newShapeCount);
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Softmax(Operand const& input) const {
+        auto result = mlGraphBuilderSoftmax(GetHandle(), input.GetHandle());
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Sub(Operand const& a, Operand const& b) const {
+        auto result = mlGraphBuilderSub(GetHandle(), a.GetHandle(), b.GetHandle());
+        return Operand::Acquire(result);
+    }
+    Operand GraphBuilder::Transpose(Operand const& input, TransposeOptions const * options) const {
+        auto result = mlGraphBuilderTranspose(GetHandle(), input.GetHandle(), reinterpret_cast<MLTransposeOptions const * >(options));
+        return Operand::Acquire(result);
+    }
+    void GraphBuilder::WebnnReference(MLGraphBuilder handle) {
+        if (handle != nullptr) {
+            mlGraphBuilderReference(handle);
+        }
+    }
+    void GraphBuilder::WebnnRelease(MLGraphBuilder handle) {
+        if (handle != nullptr) {
+            mlGraphBuilderRelease(handle);
+        }
+    }
+
+    // NamedInputs
+
+    static_assert(sizeof(NamedInputs) == sizeof(MLNamedInputs), "sizeof mismatch for NamedInputs");
+    static_assert(alignof(NamedInputs) == alignof(MLNamedInputs), "alignof mismatch for NamedInputs");
+
+    void NamedInputs::Set(char const * name, Input const * input) const {
+        mlNamedInputsSet(GetHandle(), reinterpret_cast<char const * >(name), reinterpret_cast<MLInput const * >(input));
+    }
+    void NamedInputs::WebnnReference(MLNamedInputs handle) {
+        if (handle != nullptr) {
+            mlNamedInputsReference(handle);
+        }
+    }
+    void NamedInputs::WebnnRelease(MLNamedInputs handle) {
+        if (handle != nullptr) {
+            mlNamedInputsRelease(handle);
+        }
+    }
+
+    // NamedOperands
+
+    static_assert(sizeof(NamedOperands) == sizeof(MLNamedOperands), "sizeof mismatch for NamedOperands");
+    static_assert(alignof(NamedOperands) == alignof(MLNamedOperands), "alignof mismatch for NamedOperands");
+
+    void NamedOperands::Set(char const * name, Operand const& operand) const {
+        mlNamedOperandsSet(GetHandle(), reinterpret_cast<char const * >(name), operand.GetHandle());
+    }
+    void NamedOperands::WebnnReference(MLNamedOperands handle) {
+        if (handle != nullptr) {
+            mlNamedOperandsReference(handle);
+        }
+    }
+    void NamedOperands::WebnnRelease(MLNamedOperands handle) {
+        if (handle != nullptr) {
+            mlNamedOperandsRelease(handle);
+        }
+    }
+
+    // NamedOutputs
+
+    static_assert(sizeof(NamedOutputs) == sizeof(MLNamedOutputs), "sizeof mismatch for NamedOutputs");
+    static_assert(alignof(NamedOutputs) == alignof(MLNamedOutputs), "alignof mismatch for NamedOutputs");
+
+    void NamedOutputs::Set(char const * name, Output const * output) const {
+        mlNamedOutputsSet(GetHandle(), reinterpret_cast<char const * >(name), reinterpret_cast<MLOutput const * >(output));
+    }
+    void NamedOutputs::WebnnReference(MLNamedOutputs handle) {
+        if (handle != nullptr) {
+            mlNamedOutputsReference(handle);
+        }
+    }
+    void NamedOutputs::WebnnRelease(MLNamedOutputs handle) {
+        if (handle != nullptr) {
+            mlNamedOutputsRelease(handle);
+        }
+    }
+
+    // NamedResults
+
+    static_assert(sizeof(NamedResults) == sizeof(MLNamedResults), "sizeof mismatch for NamedResults");
+    static_assert(alignof(NamedResults) == alignof(MLNamedResults), "alignof mismatch for NamedResults");
+
+    Result NamedResults::Get(char const * name) const {
+        auto result = mlNamedResultsGet(GetHandle(), reinterpret_cast<char const * >(name));
+        return Result::Acquire(result);
+    }
+    void NamedResults::WebnnReference(MLNamedResults handle) {
+        if (handle != nullptr) {
+            mlNamedResultsReference(handle);
+        }
+    }
+    void NamedResults::WebnnRelease(MLNamedResults handle) {
+        if (handle != nullptr) {
+            mlNamedResultsRelease(handle);
+        }
+    }
+
+    // Operand
+
+    static_assert(sizeof(Operand) == sizeof(MLOperand), "sizeof mismatch for Operand");
+    static_assert(alignof(Operand) == alignof(MLOperand), "alignof mismatch for Operand");
+
+    void Operand::WebnnReference(MLOperand handle) {
+        if (handle != nullptr) {
+            mlOperandReference(handle);
+        }
+    }
+    void Operand::WebnnRelease(MLOperand handle) {
+        if (handle != nullptr) {
+            mlOperandRelease(handle);
+        }
+    }
+
+    // Result
+
+    static_assert(sizeof(Result) == sizeof(MLResult), "sizeof mismatch for Result");
+    static_assert(alignof(Result) == alignof(MLResult), "alignof mismatch for Result");
+
+    const void* Result::Buffer() const {
+        auto result = mlResultBuffer(GetHandle());
+        return result;
+    }
+    uint32_t Result::BufferSize() const {
+        auto result = mlResultBufferSize(GetHandle());
+        return result;
+    }
+    const int32_t* Result::Dimensions() const {
+        auto result = mlResultDimensions(GetHandle());
+        return result;
+    }
+    uint32_t Result::DimensionsSize() const {
+        auto result = mlResultDimensionsSize(GetHandle());
+        return result;
+    }
+    void Result::WebnnReference(MLResult handle) {
+        if (handle != nullptr) {
+            mlResultReference(handle);
+        }
+    }
+    void Result::WebnnRelease(MLResult handle) {
+        if (handle != nullptr) {
+            mlResultRelease(handle);
+        }
+    }
+
+    GraphBuilder CreateGraphBuilder(Context context) {
+        return GraphBuilder::Acquire(webnnCreateGraphBuilder(context.GetHandle()));
+    }
+
+    NamedInputs CreateNamedInputs() {
+        return NamedInputs::Acquire(webnnCreateNamedInputs());
+    }
+
+    NamedOperands CreateNamedOperands() {
+        return NamedOperands::Acquire(webnnCreateNamedOperands());
+    }
+
+    NamedOutputs CreateNamedOutputs() {
+        return NamedOutputs::Acquire(webnnCreateNamedOutputs());
+    }
+
+}
diff --git a/tools/system_libs.py b/tools/system_libs.py
index b34b96f4f..8fcaef94c 100644
--- a/tools/system_libs.py
+++ b/tools/system_libs.py
@@ -1118,6 +1118,13 @@ class libwebgpu_cpp(MTLibrary):
   src_dir = ['system', 'lib', 'webgpu']
   src_files = ['webgpu_cpp.cpp']
 
+class libwebnn_cpp(MTLibrary):
+  name = 'libwebnn_cpp'
+
+  cflags = ['-std=c++11', '-O2']
+  src_dir = ['system', 'lib', 'webnn']
+  src_files = ['webnn_cpp.cpp']
+
 
 class libembind(Library):
   name = 'libembind'
@@ -1530,6 +1537,9 @@ def calculate(input_files, cxx, forced):
     if shared.Settings.USE_WEBGPU:
       add_library(system_libs_map['libwebgpu_cpp'])
 
+    if shared.Settings.USE_WEBNN:
+      add_library(system_libs_map['libwebnn_cpp'])
+
   # When LINKABLE is set the entire link command line is wrapped in --whole-archive by
   # building.link_ldd.  And since --whole-archive/--no-whole-archive processing does not nest we
   # shouldn't add any extra `--no-whole-archive` or we will undo the intent of building.link_ldd.
